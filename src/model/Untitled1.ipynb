{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b16625f-aa59-4683-bb10-5a584278c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from utils import load_data, assign_unk_english, get_word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9a9b07-48f3-4ed6-a337-8cb1f61022ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab, verbose=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    prev_tag = '--s--' \n",
    "    i = 0 \n",
    "    for word_tag in training_corpus:\n",
    "        i += 1\n",
    "        if i % 50000 == 0 and verbose:\n",
    "            print(f\"word count = {i}\")\n",
    "        word, tag = get_word_tag(word_tag, vocab, True)\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "    return emission_counts, transition_counts, tag_counts\n",
    "\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    total = 0\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        y_tup_l = y_tup.split()\n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            continue\n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                key = (pos, word)\n",
    "                if key in all_words:\n",
    "                    count = emission_counts[key]\n",
    "                    if count > count_final:\n",
    "                        count_final = count\n",
    "                        pos_final = pos\n",
    "            if pos_final == true_label:\n",
    "                num_correct += 1\n",
    "        total += 1\n",
    "    accuracy = num_correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efbf74a3-8332-45aa-8933-554d0d613472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "            if key in trans_keys:\n",
    "                count = transition_counts[key]\n",
    "            count_prev_tag = tag_counts[key[0]]\n",
    "            A[i,j] = (count + alpha) / ( count_prev_tag + (alpha * num_tags))\n",
    "    return A\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
    "               within the function it'll be treated as a list\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "            count = 0 \n",
    "            key = (all_tags[i], vocab[j]) # tuple of form (tag,word)\n",
    "            if key in emis_keys:\n",
    "                count = emission_counts[key]\n",
    "            count_tag = tag_counts[key[0]]\n",
    "            B[i,j] = (count + alpha) / (count_tag + (num_words * alpha))\n",
    "    return B\n",
    "\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    for i in range(num_tags):\n",
    "        best_probs[i,0] = math.log(A[s_idx, i]) + math.log(B[i, vocab[corpus[0]]])\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd3f3daa-9477-4229-a678-8284e45e311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transition and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    num_tags = best_probs.shape[0]\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        if i % 5000 == 0 and verbose:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "        for j in range(num_tags):\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            best_path_i = None\n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k, i-1] + math.log(A[k,j]) + math.log(B[j,vocab[test_corpus[i]]])\n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            best_paths[j,i] = best_path_i\n",
    "    return best_probs, best_paths\n",
    "\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    m = best_paths.shape[1] \n",
    "    z = [None] * m # DO NOT replace the \"None\"\n",
    "    num_tags = best_probs.shape[0]\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    pred = [None] * m\n",
    "    for k in range(num_tags):\n",
    "        if best_probs[k, -1] > best_prob_for_last_word: \n",
    "            best_prob_for_last_word = best_probs[k, -1]\n",
    "            z[m - 1] = k\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    for i in range(m-1, 0, -1):\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4582550-a4f0-41ea-b14a-4c12f53679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for prediction, y in zip(pred, y):\n",
    "        word_tag_tuple = y.split()\n",
    "        if len(word_tag_tuple) != 2: \n",
    "            continue\n",
    "        word, tag = word_tag_tuple[0], word_tag_tuple[1]\n",
    "        if prediction == tag:\n",
    "            num_correct += 1\n",
    "        total += 1\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f763428-f083-4d90-810f-9d57ee1fb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../data/WSJ_02-21.pos\"\n",
    "test_path = \"../../data/WSJ_24.pos\"\n",
    "corpus_path = \"../../data/hmm_vocab.txt\"\n",
    "\n",
    "training_corpus = load_data(train_path, False)\n",
    "y = load_data(test_path, False)\n",
    "vocab = load_data(corpus_path, True)\n",
    "\n",
    "prep = []\n",
    "for item in y:\n",
    "    word = item.split('\\t')[0]\n",
    "    if not word in vocab.keys():\n",
    "        prep.append(assign_unk_english(word))\n",
    "    else:\n",
    "        prep.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3408c568-de64-4655-9b4c-e49dbd54ae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
    "states = sorted(tag_counts.keys())\n",
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3778c1c5-462d-40b5-95cf-8e6d6eb6ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "#prep = ['economy', 'what', '--unk--', 'run']\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fd0019-c054-4df8-b095-0f7e6525f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9542\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a7336c6-dbbc-40d0-9220-77b0977c0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "from pathlib import Path\n",
    "\n",
    "def conllu_to_dataframe(filepath):\n",
    "    \"\"\"\n",
    "    Parses a CoNLL-U file and returns a pandas DataFrame with columns:\n",
    "    'sentence_id', 'word', 'lemma', 'upos', 'xpos', 'feats'\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for tokenlist in parse_incr(f):\n",
    "            for token in tokenlist:\n",
    "                if isinstance(token['id'], int):  # skip multi-word tokens\n",
    "                    data.append({\n",
    "                        \"sentence_id\": sentence_id,\n",
    "                        \"word\": token[\"form\"],\n",
    "                        \"lemma\": token[\"lemma\"],\n",
    "                        \"upos\": token[\"upos\"],  # Universal POS\n",
    "                        \"xpos\": token[\"xpos\"],  # Language-specific POS\n",
    "                        \"feats\": token[\"feats\"]  # Morphological features\n",
    "                    })\n",
    "            data.append({\n",
    "                \"sentence_id\": sentence_id,\n",
    "                \"word\": '\\n',\n",
    "                \"lemma\": '\\n',\n",
    "                \"upos\": '\\n',  # Universal POS\n",
    "                \"xpos\": '\\n',  # Language-specific POS\n",
    "                \"feats\": '\\n'  # Morphological features\n",
    "            })\n",
    "            sentence_id += 1\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_file = \"../../data/es_ancora-ud-train.conllu\"\n",
    "test_file = \"../../data/es_ancora-ud-test.conllu\"\n",
    "\n",
    "# --- Load as DataFrames ---\n",
    "df_train = conllu_to_dataframe(train_file)\n",
    "df_train['word'] = df_train['word'].apply(lambda x: x if len(x)<2 else x.replace('\"','').replace(\"'\", '').replace('(', '').replace(')', ''))\n",
    "df_test = conllu_to_dataframe(test_file)\n",
    "df_test['word'] = df_test['word'].apply(lambda x: x if len(x)<2 else x.replace('\"','').replace(\"'\", '').replace('(', '').replace(')', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3e7b4d95-f7b1-4ef6-b7bd-6ef4ae24ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_to_ptb = {\n",
    "    \"NOUN\": \"NN\",      # Common noun, singular or mass\n",
    "    \"PROPN\": \"NNP\",    # Proper noun, singular\n",
    "    \"VERB\": \"VB\",      # Base form of verb (e.g., \"run\")\n",
    "    \"AUX\": \"VB\",       # Auxiliary verb (e.g., \"have\", \"be\") simplified to base form\n",
    "    \"ADJ\": \"JJ\",       # Adjective (e.g., \"blue\")\n",
    "    \"ADV\": \"RB\",       # Adverb (e.g., \"quickly\")\n",
    "    \"PRON\": \"PRP\",     # Personal pronoun (e.g., \"he\", \"they\")\n",
    "    \"DET\": \"DT\",       # Determiner (e.g., \"the\", \"some\")\n",
    "    \"ADP\": \"IN\",       # Preposition or subordinating conjunction (e.g., \"in\", \"of\", \"that\")\n",
    "    \"CCONJ\": \"CC\",     # Coordinating conjunction (e.g., \"and\", \"but\")\n",
    "    \"SCONJ\": \"IN\",     # Subordinating conjunction, mapped to \"IN\" (same as ADP)\n",
    "    \"NUM\": \"CD\",       # Cardinal number (e.g., \"one\", \"42\")\n",
    "    \"PART\": \"RP\",      # Particle (e.g., \"up\" in \"give up\")\n",
    "    \"INTJ\": \"UH\",      # Interjection (e.g., \"uh\", \"wow\")\n",
    "    \"PUNCT\": \".\",      # Punctuation (simplified to comma â€” adjust as needed)\n",
    "    \"SYM\": \"SYM\",      # Symbol (e.g., \"$\", \"%\")\n",
    "    \"X\": \"FW\",          # Foreign word or other uncategorized token\n",
    "    \"\\n\": \"\"          # Foreign word or other uncategorized token\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b433378-2056-4007-b6c7-dc1d52ad31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['word', 'upos']]\n",
    "df_train['label'] = df_train['upos'].apply(lambda x: upos_to_ptb[x])\n",
    "\n",
    "df_test = df_test[['word', 'upos']]\n",
    "df_test['label'] = df_test['upos'].apply(lambda x: upos_to_ptb[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c0313e8-2135-46e7-953c-6c3c91463ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (df_train['word']+'\\t'+df_train['label']+'\\n').tolist()\n",
    "y = (df_test['word']+'\\t'+df_test['label']+'\\n').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07b24ff8-e33d-4989-9ea8-61f72b9b67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_list = [\"--unk_digit--\", \"--unk_punct--\", \"--unk_upper--\", \"--unk_noun--\", \"--unk_verb--\", \"--unk_adj--\", \"--unk_adv--\", \"--unk--\"]\n",
    "vocab_list = df_train['word'].unique().tolist()\n",
    "vocab_list.extend(unk_list)\n",
    "vocab = {}\n",
    "for i, word in enumerate(sorted(vocab_list)):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "521646f5-a16a-4683-aaed-ebb523e2cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = []\n",
    "for item in y:\n",
    "    word = item.split('\\t')[0]\n",
    "    if not word in vocab.keys():\n",
    "        prep.append(assign_unk_english(word))\n",
    "    else:\n",
    "        prep.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6025395a-b15f-4981-8db8-df6f5259dce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
    "states = sorted(tag_counts.keys())\n",
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7316b4e-1704-4f2e-8731-c3888100460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "58fba15d-c833-440a-b2c5-f53dadb09477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n",
      "Words processed:    35000\n",
      "Words processed:    40000\n",
      "Words processed:    45000\n",
      "Words processed:    50000\n",
      "Words processed:    55000\n"
     ]
    }
   ],
   "source": [
    "#prep = ['economy', 'what', '--unk--', 'run']\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c6d822db-4271-4709-80d9-ca85d4e4a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9361\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
